services:
  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    container_name: openmeet-postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-openmeet}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-openmeet123}
      POSTGRES_DB: ${POSTGRES_DB:-openmeet_db}
    ports:
      - '4001:5432'
    volumes:
      - postgres_data:/var/lib/postgresql/data
      # Temporarily disabled due to Windows mount issues
      # - ./infrastructure/scripts/init-db.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - openmeet-network
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -U openmeet']
      interval: 10s
      timeout: 5s
      retries: 5

  # Redis Cache
  redis:
    image: redis:7-alpine
    container_name: openmeet-redis
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD:-redis123}
    ports:
      - '6380:6379'
    volumes:
      - redis_data:/data
    networks:
      - openmeet-network
    healthcheck:
      test: ['CMD', 'redis-cli', 'ping']
      interval: 10s
      timeout: 5s
      retries: 5

  # MongoDB removed - now using PostgreSQL with pgvector for all data storage
  # mongodb:
  #   image: mongo:7
  #   container_name: openmeet-mongodb
  #   environment:
  #     MONGO_INITDB_ROOT_USERNAME: ${MONGO_USER:-openmeet}
  #     MONGO_INITDB_ROOT_PASSWORD: ${MONGO_PASSWORD:-mongo123}
  #     MONGO_INITDB_DATABASE: ${MONGO_DB:-openmeet_transcripts}
  #   ports:
  #     - '27017:27017'
  #   volumes:
  #     - mongodb_data:/data/db
  #   networks:
  #     - openmeet-network
  #   healthcheck:
  #     test: echo 'db.runCommand("ping").ok' | mongosh localhost:27017/test --quiet
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5

  # Elasticsearch for Search
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: openmeet-elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - 'ES_JAVA_OPTS=-Xms512m -Xmx512m'
    ports:
      - '9200:9200'
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    networks:
      - openmeet-network
    healthcheck:
      test: ['CMD-SHELL', 'curl -f http://localhost:9200/_cluster/health || exit 1']
      interval: 30s
      timeout: 10s
      retries: 5

  # RabbitMQ for Message Queue
  rabbitmq:
    image: rabbitmq:3-management-alpine
    container_name: openmeet-rabbitmq
    environment:
      RABBITMQ_DEFAULT_USER: ${RABBITMQ_USER:-openmeet}
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_PASSWORD:-rabbit123}
    ports:
      - '5674:5672'
      - '15674:15672'
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
    networks:
      - openmeet-network
    healthcheck:
      test: ['CMD', 'rabbitmq-diagnostics', 'ping']
      interval: 30s
      timeout: 10s
      retries: 5

  # MinIO for S3-compatible Storage
  minio:
    image: minio/minio:latest
    container_name: openmeet-minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_USER:-openmeet}
      MINIO_ROOT_PASSWORD: ${MINIO_PASSWORD:-minio123456}
    ports:
      - '9000:9000'
      - '9001:9001'
    volumes:
      - minio_data:/data
    networks:
      - openmeet-network
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:9000/minio/health/live']
      interval: 30s
      timeout: 20s
      retries: 3

  # API Service
  api:
    build:
      context: .
      dockerfile: apps/api/Dockerfile
    container_name: openmeet-api
    command: node dist/index.js
    environment:
      NODE_ENV: production
      PORT: 4000
      JWT_SECRET: ${JWT_SECRET:-dev-secret-change-in-production-min-32-chars-required}
      JWT_REFRESH_SECRET: ${JWT_REFRESH_SECRET:-dev-refresh-secret-different-from-jwt-secret}
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_PASSWORD: ${REDIS_PASSWORD:-redis123}
      DATABASE_URL: postgresql://${POSTGRES_USER:-openmeet}:${POSTGRES_PASSWORD:-openmeet123}@postgres:5432/${POSTGRES_DB:-openmeet_db}
      REDIS_URL: redis://:${REDIS_PASSWORD:-redis123}@redis:6379
      ELASTICSEARCH_URL: http://elasticsearch:9200
      RABBITMQ_URL: amqp://${RABBITMQ_USER:-openmeet}:${RABBITMQ_PASSWORD:-rabbit123}@rabbitmq:5672
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: ${MINIO_USER:-openmeet}
      S3_SECRET_KEY: ${MINIO_PASSWORD:-minio123456}
      OPENAI_API_KEY: sk-dummy-key-using-vllm-instead
      OPENAI_BASE_URL: http://vllm:8000/v1
      ALLOWED_ORIGINS: http://localhost:3000,http://localhost:3002,http://localhost:3003,http://localhost:3004,http://localhost:3005,http://localhost:3006
    ports:
      - '4000:4000'
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      vllm:
        condition: service_started
    networks:
      - openmeet-network

  # Web Frontend
  web:
    build:
      context: .
      dockerfile: apps/web/Dockerfile
    container_name: openmeet-web
    environment:
      NODE_ENV: production
      NEXT_PUBLIC_API_URL: http://localhost:4000
      NEXT_PUBLIC_WS_URL: ws://localhost:5003
    ports:
      - '3003:3000'
    depends_on:
      - api
    networks:
      - openmeet-network

  # Ollama - Local LLM inference (CPU/GPU, easy setup)
  # Alternative to vLLM for air-gapped/local deployments
  ollama:
    image: ollama/ollama:latest
    container_name: openmeet-ollama
    ports:
      - '11434:11434'
    volumes:
      - ollama_models:/root/.ollama
    networks:
      - openmeet-network
    environment:
      - OLLAMA_HOST=0.0.0.0
      # Keep model loaded in memory indefinitely (no unloading)
      - OLLAMA_KEEP_ALIVE=-1
      # Allow parallel requests
      - OLLAMA_NUM_PARALLEL=4
      # Enable flash attention for faster inference
      - OLLAMA_FLASH_ATTENTION=1
      # Use all available CPU threads
      - OLLAMA_NUM_THREADS=0
    healthcheck:
      test: ['CMD', 'ollama', 'list']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    # Uncomment for GPU support (Linux with NVIDIA only):
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

  # AI Service (Python FastAPI)
  # Handles: transcription, summarization, sentiment, diarization, chat, etc.
  ai-service:
    build:
      context: .
      dockerfile: apps/ai-service/Dockerfile
    container_name: openmeet-ai-service
    environment:
      PYTHON_ENV: ${PYTHON_ENV:-development}
      PORT: 8000
      DATABASE_URL: postgresql://${POSTGRES_USER:-openmeet}:${POSTGRES_PASSWORD:-openmeet123}@postgres:5432/${POSTGRES_DB:-openmeet_db}
      REDIS_URL: redis://:${REDIS_PASSWORD:-redis123}@redis:6379
      RABBITMQ_URL: amqp://${RABBITMQ_USER:-openmeet}:${RABBITMQ_PASSWORD:-rabbit123}@rabbitmq:5672
      # AI Provider Configuration:
      # Option 1: Cloud (OpenAI) - set OPENAI_API_KEY
      # Option 2: Local (Ollama) - uses ollama:11434
      # Option 3: Local (vLLM) - uses vllm:8000 (requires GPU)
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      OPENAI_BASE_URL: ${OPENAI_BASE_URL:-http://ollama:11434/v1}
      OPENAI_MODEL: ${OPENAI_MODEL:-llama3.2}
      LLM_MODEL: ${LLM_MODEL:-llama3.2}
      OLLAMA_URL: http://ollama:11434
      # Whisper configuration (large-v3 for production quality)
      WHISPER_PROVIDER: ${WHISPER_PROVIDER:-local}
      WHISPER_MODEL_SIZE: ${WHISPER_MODEL_SIZE:-large-v3}
      # HuggingFace token for pyannote speaker diarization
      HF_TOKEN: ${HF_TOKEN:-}
      HUGGINGFACE_TOKEN: ${HF_TOKEN:-}
      # Storage
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: ${MINIO_USER:-openmeet}
      S3_SECRET_KEY: ${MINIO_PASSWORD:-minio123456}
      CORS_ORIGINS: "*"
    ports:
      - '8888:8000'
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      minio:
        condition: service_healthy
      ollama:
        condition: service_healthy
    networks:
      - openmeet-network
    volumes:
      - ai_models:/root/.cache/huggingface
    healthcheck:
      test: ['CMD-SHELL', 'curl -f http://localhost:8000/health || exit 1']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # Real-time WebSocket Service
  realtime:
    build:
      context: .
      dockerfile: apps/realtime-service/Dockerfile
    container_name: openmeet-realtime
    environment:
      NODE_ENV: production
      PORT: 5000
      REDIS_URL: redis://:${REDIS_PASSWORD:-redis123}@redis:6379
    ports:
      - '5003:5000'
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - openmeet-network

  # Billing Microservice (Stripe integration, subscriptions, usage-based billing)
  billing:
    build:
      context: .
      dockerfile: apps/billing-service/Dockerfile
    container_name: openmeet-billing
    environment:
      NODE_ENV: production
      PORT: 4300
      JWT_SECRET: ${JWT_SECRET:-dev-secret-change-in-production-min-32-chars-required}
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_PASSWORD: ${REDIS_PASSWORD:-redis123}
      DATABASE_URL: postgresql://${POSTGRES_USER:-openmeet}:${POSTGRES_PASSWORD:-openmeet123}@postgres:5432/${POSTGRES_DB:-openmeet_db}
      # Stripe Configuration
      STRIPE_SECRET_KEY: ${STRIPE_SECRET_KEY}
      STRIPE_WEBHOOK_SECRET: ${STRIPE_WEBHOOK_SECRET}
      STRIPE_PRICE_PRO_MONTHLY: ${STRIPE_PRICE_PRO_MONTHLY}
      STRIPE_PRICE_PRO_YEARLY: ${STRIPE_PRICE_PRO_YEARLY}
      STRIPE_PRICE_BUSINESS_MONTHLY: ${STRIPE_PRICE_BUSINESS_MONTHLY}
      STRIPE_PRICE_BUSINESS_YEARLY: ${STRIPE_PRICE_BUSINESS_YEARLY}
      STRIPE_PRICE_ENTERPRISE_MONTHLY: ${STRIPE_PRICE_ENTERPRISE_MONTHLY}
      STRIPE_PRICE_ENTERPRISE_YEARLY: ${STRIPE_PRICE_ENTERPRISE_YEARLY}
      # Usage-based metering
      STRIPE_PRICE_TRANSCRIPTION_MINUTES: ${STRIPE_PRICE_TRANSCRIPTION_MINUTES}
      STRIPE_PRICE_AI_TOKENS: ${STRIPE_PRICE_AI_TOKENS}
      STRIPE_PRICE_STORAGE_GB: ${STRIPE_PRICE_STORAGE_GB}
      STRIPE_PRICE_API_CALLS: ${STRIPE_PRICE_API_CALLS}
      STRIPE_METER_TRANSCRIPTION_MINUTES: ${STRIPE_METER_TRANSCRIPTION_MINUTES}
      STRIPE_METER_AI_TOKENS: ${STRIPE_METER_AI_TOKENS}
      STRIPE_METER_STORAGE_GB: ${STRIPE_METER_STORAGE_GB}
      STRIPE_METER_API_CALLS: ${STRIPE_METER_API_CALLS}
      CORS_ORIGIN: http://localhost:3000,http://localhost:3003,http://localhost:4200
    ports:
      - '4300:4300'
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - openmeet-network
    healthcheck:
      test: ['CMD-SHELL', 'curl -f http://localhost:4300/health || exit 1']
      interval: 30s
      timeout: 10s
      retries: 5

  # Transcription Service (DISABLED - service not implemented yet)
  # transcription:
  #   build:
  #     context: .
  #     dockerfile: apps/transcription/Dockerfile
  #   container_name: openmeet-transcription
  #   environment:
  #     NODE_ENV: ${NODE_ENV:-development}
  #     PORT: 5002
  #     DATABASE_URL: postgresql://${POSTGRES_USER:-openmeet}:${POSTGRES_PASSWORD:-openmeet123}@postgres:5432/${POSTGRES_DB:-openmeet_db}
  #     MONGODB_URL: mongodb://${MONGO_USER:-openmeet}:${MONGO_PASSWORD:-mongo123}@mongodb:27017/${MONGO_DB:-openmeet_transcripts}?authSource=admin
  #     RABBITMQ_URL: amqp://${RABBITMQ_USER:-openmeet}:${RABBITMQ_PASSWORD:-rabbit123}@rabbitmq:5672
  #     S3_ENDPOINT: http://minio:9000
  #     WHISPER_API_URL: http://ai-service:5001/transcribe
  #   ports:
  #     - '5002:5002'
  #   depends_on:
  #     mongodb:
  #       condition: service_healthy
  #     rabbitmq:
  #       condition: service_healthy
  #     ai-service:
  #       condition: service_started
  #   networks:
  #     - openmeet-network
  #   volumes:
  #     - ./apps/transcription:/app
  #     - /app/node_modules

  # vLLM - High-performance LLM inference server (ONLY local AI provider)
  vllm:
    image: vllm/vllm-openai:latest
    container_name: openmeet-vllm
    ports:
      - '8000:8000'
    volumes:
      - ./ml-models:/root/.cache/huggingface
      - vllm_cache:/root/.cache/vllm
    networks:
      - openmeet-network
    environment:
      - HF_TOKEN=${HF_TOKEN}
      - HF_HOME=/root/.cache/huggingface
      - VLLM_MODEL=${VLLM_MODEL:-meta-llama/Llama-3.2-3B-Instruct}
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model ${VLLM_MODEL:-meta-llama/Llama-3.2-3B-Instruct}
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --gpu-memory-utilization 0.8
      --dtype auto
      --served-model-name ${VLLM_MODEL:-meta-llama/Llama-3.2-3B-Instruct}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ['CMD-SHELL', 'curl -f http://localhost:8000/health || exit 1']
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 120s

networks:
  openmeet-network:
    driver: bridge

volumes:
  postgres_data:
  redis_data:
  # mongodb_data: # Removed - now using PostgreSQL with pgvector
  elasticsearch_data:
  rabbitmq_data:
  minio_data:
  vllm_cache:
  ai_models:
  ollama_models:
