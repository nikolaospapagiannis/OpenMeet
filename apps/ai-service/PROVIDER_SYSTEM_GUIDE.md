# ğŸš€ AI Provider System - Complete Guide

## Overview

The **AI Provider Abstraction Layer** gives you the flexibility to use:
- â˜ï¸ **Cloud APIs**: OpenAI, Anthropic (pay-per-use)
- ğŸ  **Local Models**: Whisper, Llama, Gemma, Qwen (free, private)
- ğŸ”„ **Automatic Fallbacks**: If one fails, try the next
- ğŸ’° **Cost Optimization**: Choose cheapest or local-only
- ğŸ¯ **Smart Routing**: Best provider for each task

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Your Application                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Provider Manager                           â”‚
â”‚  - Automatic routing                                         â”‚
â”‚  - Fallback handling                                         â”‚
â”‚  - Cost estimation                                           â”‚
â”‚  - Health monitoring                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚           â”‚           â”‚
           â–¼           â–¼           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  OpenAI  â”‚ â”‚Anthropic â”‚ â”‚  Local   â”‚
    â”‚ Provider â”‚ â”‚ Provider â”‚ â”‚ Provider â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚           â”‚           â”‚
           â–¼           â–¼           â–¼
      GPT-4      Claude 3.5    Llama 3.1
      Whisper        Opus       Whisper
```

---

## Quick Start

### 1. Configuration (.env)

```bash
# OpenAI (Optional)
OPENAI_API_KEY=sk-...
OPENAI_ENABLED=true
OPENAI_PRIORITY=50

# Anthropic (Optional)
ANTHROPIC_API_KEY=sk-ant-...
ANTHROPIC_ENABLED=true
ANTHROPIC_PRIORITY=60

# Local Models (Recommended for privacy + cost savings)
LOCAL_MODELS_ENABLED=true
LOCAL_PRIORITY=100
HUGGINGFACE_TOKEN=hf_...  # Optional, for gated models
LOCAL_WHISPER_MODEL=large-v3
LOCAL_LLM_MODEL=llama-3.1-8b
LOCAL_QUANTIZATION=4bit
LOCAL_DEVICE=auto

# Provider Strategy
PROVIDER_STRATEGY=fallback  # fallback | priority | cost_optimized
```

### 2. Initialize on Startup

```python
from services.provider_config_service import initialize_provider_system

# Initialize all providers
initialize_provider_system()
```

### 3. Use in Your Code

#### Transcription (Auto-selects best provider)

```python
from services.providers import get_provider_manager, TranscriptionRequest

manager = get_provider_manager()

# Automatic provider selection
result = await manager.transcribe(
    TranscriptionRequest(
        audio_path="/path/to/audio.mp3",
        language="en",
        enable_timestamps=True,
    )
)

print(f"Transcription: {result.text}")
print(f"Provider used: {result.provider}")  # "local" or "openai"
print(f"Cost: $0.00")  # Free if using local!
```

#### Force Specific Provider

```python
from services.providers import ProviderType

# Force OpenAI Whisper
result = await manager.transcribe(
    request=TranscriptionRequest(audio_path="audio.mp3"),
    provider_type=ProviderType.OPENAI
)

# Force Local Whisper (free!)
result = await manager.transcribe(
    request=TranscriptionRequest(audio_path="audio.mp3"),
    provider_type=ProviderType.LOCAL
)
```

#### Chat Completion

```python
from services.providers import ChatRequest, ChatMessage

# Automatic provider selection
result = await manager.chat_completion(
    ChatRequest(
        messages=[
            ChatMessage(role="system", content="You are a meeting assistant."),
            ChatMessage(role="user", content="Summarize this meeting transcript..."),
        ],
        temperature=0.7,
        max_tokens=500,
    )
)

print(f"Response: {result.content}")
print(f"Provider: {result.provider}")  # "openai", "anthropic", or "local"
print(f"Tokens: {result.usage}")
```

#### Embeddings

```python
from services.providers import EmbeddingRequest

result = await manager.generate_embedding(
    EmbeddingRequest(
        input=["Meeting transcript about Q4 goals", "Action items from standup"],
        model="bge-large-en-v1.5"  # Local model (free!)
    )
)

print(f"Embeddings: {len(result.embeddings)} x {len(result.embeddings[0])} dimensions")
```

---

## Provider Strategies

### 1. **Fallback** (Recommended for Production)

Tries providers in priority order. If one fails, tries the next.

```bash
PROVIDER_STRATEGY=fallback
```

**Use case**: Maximum reliability - if OpenAI is down, use Anthropic, then Local.

### 2. **Priority**

Always uses the highest priority provider (lowest priority number).

```bash
PROVIDER_STRATEGY=priority
OPENAI_PRIORITY=50   # Use OpenAI first
LOCAL_PRIORITY=100   # Local as backup
```

**Use case**: Prefer quality (OpenAI/Anthropic) but have local fallback.

### 3. **Cost Optimized**

Always chooses the cheapest provider for each request.

```bash
PROVIDER_STRATEGY=cost_optimized
```

**Use case**: Minimize costs - uses local models whenever possible.

---

## Admin Dashboard Integration

### Get Provider Status

```python
from services.provider_config_service import get_provider_config_service

service = get_provider_config_service()
status = service.get_status()

# Returns:
{
    "provider_manager": {
        "strategy": "fallback",
        "providers": {
            "openai": {
                "enabled": true,
                "priority": 50,
                "capabilities": ["chat", "transcription", "embedding"],
                "models_count": 5
            },
            "local": {
                "enabled": true,
                "priority": 100,
                "capabilities": ["chat", "transcription", "embedding", "vision"],
                "models_count": 8
            }
        }
    },
    "total_providers": 2,
    "enabled_providers": 2
}
```

### Health Check All Providers

```python
health = await service.health_check_all()

# Returns:
{
    "openai": true,
    "anthropic": true,
    "local": true
}
```

### Update Provider Settings Dynamically

```python
# Enable/disable provider
service.enable_provider(ProviderType.LOCAL)
service.disable_provider(ProviderType.OPENAI)

# Change priority
service.set_provider_priority(ProviderType.LOCAL, priority=10)  # Higher priority

# Custom config update
service.update_provider_config(
    ProviderType.LOCAL,
    {
        "enabled": True,
        "priority": 10,
        "custom_config": {
            "whisper_model": "large-v3",
            "llm_model": "qwen2.5-7b"
        }
    },
    hot_reload=True  # Reload immediately
)
```

### Compare Costs Across Providers

```python
from services.providers import ChatRequest, ChatMessage, ModelCapability

request = ChatRequest(
    messages=[ChatMessage(role="user", content="Summarize this 1-hour meeting")],
    max_tokens=1000
)

comparison = await manager.compare_providers(request, ModelCapability.CHAT)

# Returns:
{
    "providers": [
        {
            "provider": "openai",
            "cost": {"total_cost": 0.023},
            "priority": 50
        },
        {
            "provider": "anthropic",
            "cost": {"total_cost": 0.018},
            "priority": 60
        },
        {
            "provider": "local",
            "cost": {"total_cost": 0.0},
            "priority": 100,
            "note": "FREE!"
        }
    ],
    "cheapest": "local"
}
```

---

## Cost Comparison

| Task | OpenAI | Anthropic | Local | Savings |
|------|--------|-----------|-------|---------|
| **Transcribe 1hr audio** | $0.36 | N/A | **$0.00** | 100% |
| **Summarize transcript (1000 tokens)** | $0.02 | $0.015 | **$0.00** | 100% |
| **Generate embeddings (1000 inputs)** | $0.13 | N/A | **$0.00** | 100% |
| **100 meetings/month** | ~$50 | ~$40 | **$0.00** | **100%** |

**Annual Savings with Local Models**: **$600-1000+**

---

## Local Model Setup

### Hardware Requirements

| Model | VRAM | RAM | Speed |
|-------|------|-----|-------|
| **Whisper Large-v3** | 6GB | 8GB | 5x faster than API |
| **Llama 3.1 8B (4-bit)** | 6GB | 16GB | ~30 tokens/sec |
| **Qwen 2.5 7B (4-bit)** | 5GB | 16GB | ~35 tokens/sec |
| **Gemma 2 9B (4-bit)** | 7GB | 18GB | ~25 tokens/sec |

### Installation

```bash
# Install dependencies
pip install faster-whisper transformers torch accelerate bitsandbytes sentence-transformers

# GPU support (CUDA)
pip install torch --index-url https://download.pytorch.org/whl/cu118

# Download models (first run)
python -m services.providers.local_provider
```

### Docker Deployment

See `docker-compose.local-ml.yml` for complete setup with GPU support.

---

## API Endpoints for Admin Dashboard

### GET /api/v1/providers/status

Get all provider status.

```json
{
  "strategy": "fallback",
  "providers": {
    "openai": { "enabled": true, "priority": 50 },
    "local": { "enabled": true, "priority": 100 }
  }
}
```

### POST /api/v1/providers/update

Update provider configuration.

```json
{
  "provider": "local",
  "enabled": true,
  "priority": 10,
  "custom_config": {
    "whisper_model": "large-v3"
  }
}
```

### GET /api/v1/providers/health

Health check all providers.

```json
{
  "openai": true,
  "anthropic": false,
  "local": true
}
```

### POST /api/v1/providers/compare-cost

Compare costs for a request.

```json
{
  "request_type": "chat",
  "messages": [...],
  "max_tokens": 1000
}
```

Response:
```json
{
  "cheapest": "local",
  "providers": [
    {"provider": "openai", "cost": 0.023},
    {"provider": "local", "cost": 0.0}
  ]
}
```

---

## Best Practices

### 1. **Development**: Use Local Models
```bash
LOCAL_MODELS_ENABLED=true
LOCAL_PRIORITY=10  # Highest priority
OPENAI_ENABLED=false
```
âœ… Zero costs
âœ… Full privacy
âœ… Faster iteration

### 2. **Production**: Hybrid Approach
```bash
# Local for transcription (fast + free)
LOCAL_PRIORITY=10
LOCAL_WHISPER_MODEL=large-v3

# Cloud for complex summarization (better quality)
OPENAI_PRIORITY=50
ANTHROPIC_PRIORITY=60

# Strategy: fallback
PROVIDER_STRATEGY=fallback
```
âœ… Best quality where it matters
âœ… Cost savings on transcription
âœ… Automatic fallbacks

### 3. **High-Privacy**: Local Only
```bash
LOCAL_MODELS_ENABLED=true
OPENAI_ENABLED=false
ANTHROPIC_ENABLED=false
```
âœ… All data stays on your servers
âœ… HIPAA/GDPR compliant
âœ… Zero cloud dependencies

---

## Troubleshooting

### Provider Not Loading

```python
# Check health
health = await service.health_check_all()
print(health)

# Check config
config = service.get_provider_config(ProviderType.LOCAL)
print(config)
```

### Out of Memory (Local Models)

```bash
# Use smaller models
LOCAL_LLM_MODEL=phi-3.5-mini  # Only 7.6GB

# Or use CPU (slower but no VRAM needed)
LOCAL_DEVICE=cpu
```

### Slow Local Inference

```bash
# Use quantization
LOCAL_QUANTIZATION=4bit  # 75% memory savings, minimal quality loss

# Use smaller context
# Reduce max_tokens in requests
```

---

## Next Steps

1. âœ… Configure providers in `.env`
2. âœ… Initialize on app startup
3. âœ… Use `manager.transcribe()` and `manager.chat_completion()`
4. âœ… Build admin dashboard with provider settings
5. âœ… Monitor costs and usage
6. âœ… Fine-tune local models on your meeting data (advanced)

---

## Support

For issues or questions:
- Check logs: `logger.info("Provider status")`
- Health check: `await service.health_check_all()`
- Status: `service.get_status()`

**You now have a production-ready, cost-optimized, privacy-focused AI provider system! ğŸ‰**
