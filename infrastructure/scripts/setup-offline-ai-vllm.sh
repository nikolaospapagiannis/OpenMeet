#!/bin/bash
#
# Automated vLLM Setup - Download models and start vLLM for maximum performance
#

set -e

echo "üöÄ OpenMeet v2 - Automated vLLM Setup"
echo "===================================="
echo ""
echo "vLLM is the FASTEST option for local AI inference!"
echo "  - 2-3x faster than Ollama"
echo "  - Better GPU utilization"
echo "  - OpenAI-compatible API"
echo ""

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Check if running from project root
if [ ! -f "docker-compose.yml" ]; then
    echo -e "${RED}‚ùå Error: Must run from project root${NC}"
    exit 1
fi

# Check HuggingFace token
if [ -z "$HF_TOKEN" ]; then
    echo -e "${YELLOW}‚ö†Ô∏è  HF_TOKEN not set in environment${NC}"
    echo "   Checking .env file..."

    if [ -f ".env" ]; then
        source .env
        if [ -n "$HF_TOKEN" ] && [ "$HF_TOKEN" != "your-hf-token-here" ]; then
            echo -e "${GREEN}‚úÖ Found HF_TOKEN in .env${NC}"
        else
            echo -e "${RED}‚ùå HF_TOKEN not configured in .env${NC}"
            echo "   Get your token from: https://huggingface.co/settings/tokens"
            exit 1
        fi
    else
        echo -e "${RED}‚ùå .env file not found${NC}"
        exit 1
    fi
fi

echo ""
echo "üîç Step 1: Checking GPU availability"
echo "-------------------------------------"

HAS_GPU=false

# Check for NVIDIA GPU (Linux/WSL)
if command -v nvidia-smi &> /dev/null; then
    if nvidia-smi &> /dev/null; then
        echo -e "${GREEN}‚úÖ NVIDIA GPU detected!${NC}"
        nvidia-smi --query-gpu=name,memory.total --format=csv,noheader
        HAS_GPU=true
    else
        echo -e "${YELLOW}‚ö†Ô∏è  nvidia-smi found but not working${NC}"
    fi
else
    # Check for GPU on Windows
    if command -v wmic &> /dev/null; then
        if wmic path win32_VideoController get name | grep -i "nvidia\|amd" > /dev/null; then
            echo -e "${GREEN}‚úÖ GPU detected (Windows)${NC}"
            wmic path win32_VideoController get name
            HAS_GPU=true
            echo ""
            echo -e "${YELLOW}‚ö†Ô∏è  Note: vLLM requires WSL2 + NVIDIA Docker on Windows${NC}"
            echo "   See: https://docs.nvidia.com/cuda/wsl-user-guide/index.html"
        fi
    fi
fi

if [ "$HAS_GPU" = false ]; then
    echo -e "${YELLOW}‚ö†Ô∏è  No GPU detected${NC}"
    echo ""
    echo "vLLM requires a GPU for best performance."
    echo ""
    echo "Options:"
    echo "  1. Continue with CPU (slower, not recommended for vLLM)"
    echo "  2. Use Ollama instead (CPU-friendly)"
    echo "  3. Cancel and install GPU drivers"
    echo ""
    read -p "Choice [1/2/3]: " -n 1 -r
    echo

    case $REPLY in
        2)
            echo ""
            echo "Switching to Ollama setup..."
            exec ./infrastructure/scripts/setup-offline-ai.sh
            ;;
        3)
            echo "Cancelled. Install GPU drivers and try again."
            exit 0
            ;;
        *)
            echo -e "${YELLOW}‚ö†Ô∏è  Continuing with CPU (expect slower performance)${NC}"
            ;;
    esac
fi

echo ""
echo "üì¶ Step 2: Creating directories"
echo "--------------------------------"
mkdir -p ml-models
mkdir -p ml-models/.cache/huggingface
echo -e "${GREEN}‚úÖ Directories created${NC}"

echo ""
echo "üì• Step 3: Downloading models from HuggingFace"
echo "-----------------------------------------------"
echo ""
echo "This will download models optimized for vLLM:"
echo ""
echo "  ${BLUE}LLM Models (choose one):${NC}"
echo "  1. ${GREEN}Llama 3.2 3B Instruct${NC} (~6GB) - Recommended, best quality"
echo "  2. ${YELLOW}Qwen 2.5 3B Instruct${NC} (~6GB) - Alternative, good quality"
echo "  3. ${YELLOW}Phi-3 Mini${NC} (~8GB) - Microsoft, instruction-tuned"
echo ""
echo "  ${BLUE}Whisper Model (speech-to-text):${NC}"
echo "  ‚Ä¢ ${GREEN}Whisper Small${NC} (~1GB) - Recommended"
echo ""
echo "  ${BLUE}Embeddings Model:${NC}"
echo "  ‚Ä¢ ${GREEN}MiniLM-L6-v2${NC} (~90MB) - Fast and lightweight"
echo ""
echo "Total download: ~7-8GB"
echo ""

read -p "Which LLM model? [1=Llama/2=Qwen/3=Phi] (default: 1): " -n 1 -r
echo
LLM_CHOICE=${REPLY:-1}

# Set model based on choice
case $LLM_CHOICE in
    2)
        LLM_MODEL="qwen-2.5-3b"
        LLM_REPO="Qwen/Qwen2.5-3B-Instruct"
        ;;
    3)
        LLM_MODEL="phi-3-mini"
        LLM_REPO="microsoft/Phi-3-mini-4k-instruct"
        ;;
    *)
        LLM_MODEL="llama-3.2-3b"
        LLM_REPO="meta-llama/Llama-3.2-3B-Instruct"
        ;;
esac

echo ""
echo "Selected: ${GREEN}${LLM_MODEL}${NC}"
echo ""

# Detect Python command (python3 on Linux/Mac, python on Windows)
PYTHON_CMD="python3"
if command -v python &> /dev/null; then
    PYTHON_VERSION=$(python --version 2>&1)
    if [[ $PYTHON_VERSION == *"Python 3"* ]]; then
        PYTHON_CMD="python"
    fi
fi

echo "Using Python: $PYTHON_CMD ($($PYTHON_CMD --version 2>&1))"

# Install Python dependencies
echo "üì¶ Installing Python dependencies..."
$PYTHON_CMD -m pip install -q huggingface-hub || {
    echo -e "${RED}‚ùå Failed to install huggingface-hub${NC}"
    echo "Try: $PYTHON_CMD -m pip install --user huggingface-hub"
    exit 1
}

echo ""
echo "‚è≥ Downloading models (this will take 15-20 minutes)..."
echo ""

# Download LLM model
echo "${BLUE}[1/3]${NC} Downloading ${LLM_MODEL}..."
$PYTHON_CMD infrastructure/scripts/download-models.py download --model "${LLM_MODEL}" || {
    echo -e "${RED}‚ùå Failed to download ${LLM_MODEL}${NC}"
    exit 1
}

# Download Whisper
echo ""
echo "${BLUE}[2/3]${NC} Downloading Whisper Small..."
$PYTHON_CMD infrastructure/scripts/download-models.py download --model whisper-small || {
    echo -e "${YELLOW}‚ö†Ô∏è  Whisper download failed, trying Tiny instead...${NC}"
    $PYTHON_CMD infrastructure/scripts/download-models.py download --model whisper-tiny
}

# Download embeddings
echo ""
echo "${BLUE}[3/3]${NC} Downloading MiniLM embeddings..."
$PYTHON_CMD infrastructure/scripts/download-models.py download --model all-minilm || {
    echo -e "${YELLOW}‚ö†Ô∏è  MiniLM download failed${NC}"
}

echo ""
echo -e "${GREEN}‚úÖ All models downloaded!${NC}"

echo ""
echo "‚öôÔ∏è  Step 4: Configuring vLLM"
echo "----------------------------"

# Update .env with vLLM settings
if [ -f ".env" ]; then
    # Backup original .env
    cp .env .env.backup

    # Update AI provider settings
    sed -i "s|^AI_PROVIDER=.*|AI_PROVIDER=vllm|g" .env
    sed -i "s|^VLLM_MODEL=.*|VLLM_MODEL=${LLM_REPO}|g" .env

    echo -e "${GREEN}‚úÖ Configuration updated in .env${NC}"
    echo "   AI_PROVIDER=vllm"
    echo "   VLLM_MODEL=${LLM_REPO}"
else
    echo -e "${RED}‚ùå .env file not found${NC}"
    exit 1
fi

echo ""
echo "üê≥ Step 5: Starting vLLM container"
echo "-----------------------------------"

# Check if nvidia-docker is available
if [ "$HAS_GPU" = true ]; then
    if ! docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi &> /dev/null; then
        echo -e "${YELLOW}‚ö†Ô∏è  NVIDIA Docker runtime not properly configured${NC}"
        echo "   vLLM will start but may not use GPU"
        echo ""
        echo "To fix, install nvidia-container-toolkit:"
        echo "  https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html"
        echo ""
    fi
fi

# Start vLLM
echo "Starting vLLM container..."
docker-compose up -d vllm

echo ""
echo "‚è≥ Waiting for vLLM to load model (this takes 1-2 minutes)..."
echo ""

# Wait for vLLM to be ready
MAX_WAIT=180
WAITED=0
while [ $WAITED -lt $MAX_WAIT ]; do
    if curl -s http://localhost:8000/health > /dev/null 2>&1; then
        echo -e "${GREEN}‚úÖ vLLM is ready!${NC}"
        break
    fi
    echo -n "."
    sleep 5
    WAITED=$((WAITED + 5))
done

if [ $WAITED -ge $MAX_WAIT ]; then
    echo ""
    echo -e "${RED}‚ùå vLLM failed to start within timeout${NC}"
    echo ""
    echo "Check logs:"
    echo "  docker-compose logs vllm"
    exit 1
fi

echo ""
echo ""
echo "‚úÖ Step 6: Verifying installation"
echo "----------------------------------"

# Test vLLM API
echo -n "Testing vLLM API... "
if curl -s http://localhost:8000/v1/models > /dev/null; then
    echo -e "${GREEN}‚úÖ${NC}"
else
    echo -e "${RED}‚ùå${NC}"
fi

# List available models
echo ""
echo "üìä Available Models:"
MODELS=$(curl -s http://localhost:8000/v1/models | grep -o '"id":"[^"]*"' | cut -d'"' -f4)
echo -e "${GREEN}${MODELS}${NC}"

echo ""
echo "üéâ vLLM Setup Complete!"
echo "========================"
echo ""
echo "‚úÖ What's ready:"
echo "  - vLLM running on port 8000"
echo "  - Model: ${LLM_REPO}"
echo "  - Whisper Small/Tiny for transcription"
echo "  - MiniLM for embeddings"
echo "  - OpenAI-compatible API"
echo ""
echo "üìä Performance:"
if [ "$HAS_GPU" = true ]; then
    echo "  - ${GREEN}GPU acceleration enabled${NC}"
    echo "  - Expected: 30-50 tokens/second"
    echo "  - 2-3x faster than Ollama"
else
    echo "  - ${YELLOW}CPU mode (slower)${NC}"
    echo "  - Expected: 5-10 tokens/second"
fi
echo ""
echo "üìã Next Steps:"
echo ""
echo "  1. Test vLLM:"
echo "     ${BLUE}curl http://localhost:8000/v1/chat/completions -H 'Content-Type: application/json' -d '{\"model\":\"${LLM_REPO}\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello!\"}]}'${NC}"
echo ""
echo "  2. Start application services:"
echo "     ${BLUE}docker-compose up -d api web ai-service realtime${NC}"
echo ""
echo "  3. Open web UI:"
echo "     ${BLUE}http://localhost:3003${NC}"
echo ""
echo "  4. Load Chrome extension:"
echo "     ${BLUE}chrome://extensions/ ‚Üí Load unpacked ‚Üí apps/chrome-extension/${NC}"
echo ""
echo "üí∞ Cost Savings:"
echo "  - vLLM: ${GREEN}\$0/month${NC}"
echo "  - OpenAI: \$920/month"
echo "  - ${GREEN}Savings: 100%!${NC}"
echo ""
echo "üéØ Your system is now configured for maximum performance offline AI! üöÄ"
echo ""
